# cs_internship_journey
The projects I work on in the CS internship program.

## 1. **Zip Extractor Project**
In Step 4, I had to write a program to extract and read the Forex data.  
[Zip Extractor from Scratch](https://github.com/nargesghan/cs_internship_journey/tree/main/zip%20extractor%20from%20scratch)

### Modules Used:
- `os`
- `zipfile`

### Setup and Run Instructions:
1. Ensure you have Python installed on your system.
2. Clone the repository:
   ```bash
   git clone https://github.com/nargesghan/cs_internship_journey.git
   cd cs_internship_journey/zip\ extractor\ from\ scratch
   ```
3. Run the zip extractor script:
   ```bash
   python zip_extractor.py
   ```

### Running Tests:
1. Navigate to the `zip extractor from scratch` directory:
   ```bash
   cd cs_internship_journey/zip\ extractor\ from\ scratch
   ```
2. Run the tests using `unittest`:
   ```bash
   python -m unittest zip_extractor_test.py
   ```

## 2. **time series**
 In Step 5, I worked on the Forex data mentioned earlier and calculated the moving average of this data. Additionally, I practiced with the Seattle Bicycle Counts using Python Data Science Handbook by Jake VanderPlas.
Modules Used:

### modules Used:
- `zipfile`
- `pandas`
- `numpy`

### Setup and Run Instructions:
1. Ensure you have Python installed on your system.
2. Clone the repository:
   ```bash
   git clone https://github.com/nargesghan/cs_internship_journey.git
   cd cs_internship_journey/time\ series(forex\ and\ bicycle\ counter)
   ```
3. Install the required dependencies:
   ```bash
   pip install pandas numpy
   ```
4. Run the Jupyter notebook:
   ```bash
   jupyter notebook Forex_Historical_Data-bicycle-counter.ipynb
   ```

## 3. **pandas-numpy-matplotlib**

In step 5, I prepared three learning materials for machine learning libraries. I used their cheat sheets and documentation as sources. These notebooks contain useful methods from these libraries, along with examples and sufficient explanations.

### Setup and Run Instructions:
1. Ensure you have Python installed on your system.
2. Clone the repository:
   ```bash
   git clone https://github.com/nargesghan/cs_internship_journey.git
   cd cs_internship_journey/numpy_pandas_matplotlib
   ```
3. Install the required dependencies:
   ```bash
   pip install numpy pandas matplotlib
   ```
4. Run the Jupyter notebooks:
   ```bash
   jupyter notebook numpy_cheat_sheet.ipynb
   jupyter notebook pandas_cheat_sheet.ipynb
   jupyter notebook matplotlib_cheat_sheet.ipynb
   ```

### Running Tests:
1. Navigate to the `numpy_pandas_matplotlib` directory:
   ```bash
   cd cs_internship_journey/numpy_pandas_matplotlib
   ```
2. Run the tests using `unittest`:
   ```bash
   python -m unittest test_numpy_cheat_sheet.py
   ```

## 4. **simple-classification**
In step 6, I implemented a simple one-layer neural network using the NumPy library. I used three different learning algorithms for this task: Perceptron, Adaline, and Adaline with stochastic gradient descent. I trained all of these algorithms using the Iris dataset. If you're interested in learning how to implement a single-layer neural network from scratch, don't miss this notebook.

### Setup and Run Instructions:
1. Ensure you have Python installed on your system.
2. Clone the repository:
   ```bash
   git clone https://github.com/nargesghan/cs_internship_journey.git
   cd cs_internship_journey/simple-classification
   ```
3. Install the required dependencies:
   ```bash
   pip install numpy pandas matplotlib
   ```
4. Run the Jupyter notebook:
   ```bash
   jupyter notebook perceptron.ipynb
   ```

## 5. **Dimensionality Reduction**
In this project, I worked on data preprocessing and dimensionality reduction techniques.

### Setup and Run Instructions:
1. Ensure you have Python installed on your system.
2. Clone the repository:
   ```bash
   git clone https://github.com/nargesghan/cs_internship_journey.git
   cd cs_internship_journey/Dimensionality\ Reduction
   ```
3. Install the required dependencies:
   ```bash
   pip install numpy pandas matplotlib scikit-learn
   ```
4. Run the Jupyter notebook:
   ```bash
   jupyter notebook Compressing_Data_via_Dimensionality_Reduction.ipynb
   ```

### Running Tests:
1. Navigate to the `Dimensionality Reduction` directory:
   ```bash
   cd cs_internship_journey/Dimensionality\ Reduction
   ```
2. Run the tests using `unittest`:
   ```bash
   python -m unittest test_dimensionality_reduction.py
   ```
